{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "face-anti-spoofing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIsDlItPetQ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6136c228-f2c7-47d1-c588-5bc14b4901f0"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import shutil # high-level operations on files\n",
        "from tqdm import tqdm # Progress bar and status logging\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "import cv2 # computer vision algorithms\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras import utils\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dquNN4h1qIi6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "69c147cb-df5c-45cb-8a8a-cd59af93b378"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkf5LLXAetRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configuration\n",
        "\n",
        "DATASET_DIR = 'C:\\\\Users\\\\yash\\\\Desktop\\\\kaggle\\\\real-and-fake-face-detection\\\\real_and_fake_face_detection/real_and_fake_face'\n",
        "TRAIN_DIR = 'C:\\\\Users\\\\yash\\\\Desktop\\\\kaggle\\\\train_dataset'\n",
        "TEST_DIR = 'C:\\\\Users\\\\yash\\\\Desktop\\\\kaggle\\\\test_dataset'\n",
        "\n",
        "RATE = 0.2 # splitting proportion for training and test datasets\n",
        "\n",
        "# Parameters for Grid Search\n",
        "\n",
        "N_EPOCHS = [1] #[20, 40, 100, 200]\n",
        "OPTIMIZERS = ['adam'] #['adam', 'rmsprop', 'SGD']\n",
        "DROPOUT_RATES =  [0.4]\n",
        "LOSS_FUNCTIONS = ['binary_crossentropy']  #['sparse_categorical_crossentropy', 'kullback_leibler_divergence']    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqz7pGzbetRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir(TRAIN_DIR)\n",
        "os.mkdir(TRAIN_DIR+'\\\\fake')\n",
        "os.mkdir(TRAIN_DIR+'\\\\real')\n",
        "\n",
        "os.mkdir(TEST_DIR)\n",
        "os.mkdir(TEST_DIR+'\\\\fake')\n",
        "os.mkdir(TEST_DIR+'\\\\real')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Daa7R1LKetRU",
        "colab_type": "text"
      },
      "source": [
        "Updated folder structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5-U4Ff4etRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dirname, _, filenames in os.walk('C:\\\\Users\\\\yash\\\\Desktop\\\\kaggle'):\n",
        "    print (dirname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rtyeHneetRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split folders with images into training, validation and test folders.\n",
        "# OPTION 1 (using split-folders)\n",
        "\n",
        "#pip install split-folders\n",
        "\n",
        "#import split_folders\n",
        "\n",
        "# Split with a ratio.\n",
        "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
        "#split_folders.ratio('input_folder', output=\"output\", seed=1337, ratio=(.8, .1, .1)) # default values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSYh7wUqetRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "255d6d95-a7e7-455a-bd20-96bba71545b4"
      },
      "source": [
        "# Split image files into test and training set \n",
        "# OPTION 2 (copying files into newly created folders)\n",
        "files_real = os.listdir(f'{DATASET_DIR}\\\\training_real')\n",
        "files_fake = os.listdir(f'{DATASET_DIR}\\\\training_fake')\n",
        "\n",
        "\n",
        "# sample from each class to create a test set\n",
        "np.random.seed(0)\n",
        "files_real_test = np.random.choice(\n",
        "    files_real,\n",
        "    size=round(len(files_real) * RATE),\n",
        "    replace=False,\n",
        "    p=None)\n",
        "\n",
        "files_real_train = list(set(files_real) - set(files_real_test)) #[file for file in files_real if file not in files_real_test] \n",
        "\n",
        "files_fake_test = np.random.choice(\n",
        "    files_fake,\n",
        "    size=round(len(files_fake) * RATE),\n",
        "    replace=False,\n",
        "    p=None)\n",
        "\n",
        "files_fake_train = list(set(files_fake) - set(files_fake_test)) #[file for file in files_fake if file not in files_fake_test] \n",
        "\n",
        "for file in files_real_train:\n",
        "    shutil.copyfile(DATASET_DIR+'/training_real/'+file, TRAIN_DIR+'/real/'+file) \n",
        "\n",
        "for file in files_fake_train:\n",
        "    shutil.copyfile(DATASET_DIR+'/training_fake/'+file, TRAIN_DIR+'/fake/'+file) \n",
        "\n",
        "for file in files_real_test:\n",
        "    shutil.copyfile(DATASET_DIR+'/training_real/'+file, TEST_DIR+'/real/'+file) \n",
        "\n",
        "for file in files_fake_test:\n",
        "    shutil.copyfile(DATASET_DIR+'/training_fake/'+file, TEST_DIR+'/fake/'+file) \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9d7671bcf432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATASET_DIR}\\\\training_real'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfiles_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATASET_DIR}\\\\training_fake'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# sample from each class to create a test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yash\\\\Desktop\\\\kaggle\\\\real-and-fake-face-detection\\\\real_and_fake_face_detection/real_and_fake_face\\\\training_real'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrcsP0S3etRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "676966e5-bf63-409d-a41f-810629a64928"
      },
      "source": [
        "train_samples = sum([len(files) for r, d, files in os.walk(TRAIN_DIR)])\n",
        "test_samples = sum([len(files) for r, d, files in os.walk(TEST_DIR)])\n",
        "print('Number of training images: {} \\nNumber of test images: {}'.format(train_samples, test_samples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training images: 0 \n",
            "Number of test images: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL3oI8ELetRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load and show an image with Pillow\n",
        "# from PIL import Image\n",
        "# image = Image.open('/kaggle/test_dataset/fake/hard_39_1111.jpg')\n",
        "# # some details about the image\n",
        "# print(image.format)\n",
        "# print(image.mode)\n",
        "# print(image.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ipr4CLdetRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_images(path, img_shape=(64, 64)):\n",
        " \n",
        "    '''\n",
        "    Returns a np array of images and labels from path\n",
        "    Images must be stored in path/class1, path/class2\n",
        "    '''\n",
        "    main_path = path\n",
        "    y = []\n",
        "    list = [name for name in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, name))]\n",
        "    print(list)\n",
        "    image_collection = []\n",
        "    for idx,folder in enumerate(list):\n",
        " \n",
        "        label = idx\n",
        "        \n",
        "        sub_list = sorted(os.listdir(os.path.join(main_path,folder)))\n",
        " \n",
        "        for i in tqdm(range(1, len(sub_list))):\n",
        "            image_path = os.path.join(main_path, folder, sub_list[i])\n",
        "            read_image = cv2.imread(image_path)\n",
        "            image_resized = cv2.resize(read_image, img_shape, interpolation=cv2.INTER_AREA)\n",
        " \n",
        "            image = np.float32(image_resized)\n",
        "            image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F) #Change alpha, beta according to the preprocessing you desire\n",
        "            \n",
        "            image_collection.append(image)\n",
        "            \n",
        "            y.append(label)\n",
        " \n",
        "    y = np.array(y)\n",
        "    y = utils.to_categorical(y,num_classes=len(list))\n",
        " \n",
        "    return image_collection, y[:,0] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OEgv_m4etRp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e979bd15-a71b-4c80-b817-db5e0483908f"
      },
      "source": [
        "# Preparing test and trainng datasets\n",
        "X_train,y_train = get_images(\"/content/drive/My Drive/kaggle/train_dataset\",img_shape=(64,64))\n",
        "#X_test,y_test = get_images(\"/content/drive/My Drive/kaggle/test_dataset\",img_shape=(64,64))\n",
        "#dict_data1 = np.load('/content/drive/My Drive/kaggle/xtrain.npz')\n",
        "dict_data2 = np.load('/content/drive/My Drive/Colab Notebooks/xtest.npz')\n",
        "dict_data3 = np.load('/content/drive/My Drive/Colab Notebooks/ytrain.npz')\n",
        "dict_data4 = np.load('/content/drive/My Drive/Colab Notebooks/ytest.npz')\n",
        "#X_train = dict_data1['arr_0']\n",
        "X_test = dict_data2['arr_0']\n",
        "y_train = dict_data3['arr_0']\n",
        "y_test = dict_data4['arr_0']\n",
        "X_train = np.array(X_train)\n",
        "#X_test = np.array(X_test)\n",
        "#np.savez_compressed('xtest.npz',X_test)\n",
        "np.savez_compressed('xtrain.npz',X_train)\n",
        "#np.savez_compressed('ytest.npz',y_test)\n",
        "#np.savez_compressed('ytrain.npz',y_train)\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 6/767 [00:00<00:12, 59.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['fake', 'real']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 767/767 [03:47<00:00,  3.37it/s]\n",
            "100%|██████████| 864/864 [04:01<00:00,  3.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRIdZIVGetRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "eae207ec-5130-4026-89d5-5d7e35a0005f"
      },
      "source": [
        "print('Training set', X_train.shape)\n",
        "print('Test set', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (1631, 64, 64, 3)\n",
            "Test set (406, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6AD1w3detRu",
        "colab_type": "text"
      },
      "source": [
        "We don't have too much data to train the network. \n",
        "One of possible workarounds is to use ImageDataGenerator.\n",
        "On the one hand, it does allow us to generate additional examples. On the other hand, all of these examples are based on a too small dataset and the network still cannot generalize to data it was never trained on \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLRg0FPDetRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# train_datagen = ImageDataGenerator(shear_range = 0.2,\n",
        "#                                    zoom_range = 0.2,\n",
        "#                                    horizontal_flip = True)\n",
        "# batches = 0\n",
        "# for _ in train_datagen.flow_from_directory(directory=TRAIN_DIR, class_mode='binary', batch_size = 32, save_prefix='real_aug', save_to_dir=TRAIN_DIR+'/real', save_format='jpeg'):\n",
        "#     batches += 1\n",
        "#     if batches >= number_of_real_images / 32:\n",
        "#         break # to stop the indefinite generator loop\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oytlk1twetR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shuffle training examples\n",
        "X_train, y_train = shuffle(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtTBuP_ietR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "dda13e81-53ec-4e34-eeaa-24fb8949933d"
      },
      "source": [
        "#def build_classifier(optimizer, dropout, loss):\n",
        "classifier = Sequential() # Initialising the CNN    \n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu')) \n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2))) \n",
        "classifier.add(Dropout(0.4))\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))  \n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "classifier.add(Dropout(0.4))\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))  \n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "classifier.add(Dropout(0.4))\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid')) #'tanh'))\n",
        "\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "#plot_model(classifier, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "classifier.summary()\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 29, 29, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 12, 12, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               147584    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 167,105\n",
            "Trainable params: 167,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYIsAJPCetSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f2843bd-5e72-4cc4-ae13-6acf4d7b9a35"
      },
      "source": [
        "#preds = classifier.evaluate(x=X_test,y=y_test)\n",
        "### END CODE HERE ###\n",
        "print()\n",
        "#print (\"Loss = \" + str(preds[0]))\n",
        "#print (\"Test Accuracy = \" + str(preds[1]))    \n",
        "#    return classifier\n",
        "\n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "\n",
        "grid_parameters = {'epochs': N_EPOCHS,\n",
        "                  'optimizer': OPTIMIZERS,\n",
        "                  'dropout': DROPOUT_RATES,                  \n",
        "                  'loss':LOSS_FUNCTIONS                        \n",
        "                  }\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = grid_parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hV-OvfSYmIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search = grid_search.fit(X_train, y_train, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwNc9xwKk2ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}